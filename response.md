1. in order both of the models to output the next token properly, their respective tokenizers need to get used. The problem becomes apparent when you get logits from each model they dont line up. A simply approahc that won't increase complexity too much would a simply fully connected net work from the smaller voab size vector of tokens to the bigger one. We could essentially map which tokens match up with from one tokenization model to the other, and use that wieghted connections to map tokens from model to the other during decoding. 
2. I don't think contrastive decoding is used in practice primarily because contrastive decoding takes sibstantially more time than normal decoding practices because for each token, two forward passes need to get done. Most chat bots these day are hinged upon being able to fast inference and decoding and this method slows it down. It also adds a bit more comlexity into the mix has you ow have douible the amount of choices to make, as you need to choose hyper parameters for two different models now